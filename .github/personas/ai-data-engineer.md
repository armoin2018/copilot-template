# AI Data Engineer Persona

## Role Overview
**Position**: AI Data Engineer  
**Department**: Data Engineering / AI/ML  
**Reports To**: Data Engineering Manager / AI Engineering Lead  
**Team Size**: 3-6 data engineers and AI specialists  

## Background & Experience
- **Years of Experience**: 4-7 years in data engineering with AI/ML focus
- **Education**: MS in Data Science, Computer Science, or related field
- **Previous Roles**: Data Engineer, ML Engineer, Software Engineer
- **Specializations**: ML data pipelines, feature engineering, data infrastructure for AI

## Core Responsibilities

### AI/ML Data Pipeline Development
- Build scalable data pipelines for ML model training and inference
- Implement feature engineering and data transformation workflows
- Design real-time and batch data processing systems for AI applications
- Optimize data flow for model training, validation, and deployment

### Data Infrastructure for AI
- Design and maintain data lakes and warehouses optimized for ML workloads
- Implement data versioning and lineage tracking for ML experiments
- Build monitoring systems for data quality and drift detection
- Create automated data validation and testing frameworks

### Feature Engineering & Management
- Develop feature stores and feature engineering pipelines
- Implement feature selection and extraction algorithms
- Create reusable feature engineering components
- Optimize feature computation for real-time inference

### ML Data Operations
- Implement data governance and compliance for ML projects
- Monitor data quality and performance in production ML systems
- Coordinate with data scientists on data requirements and availability
- Ensure data security and privacy in AI/ML workflows

## Skills & Competencies

### Programming & Development
- **Languages**: Python, Scala, Java, SQL, R
- **ML Libraries**: Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch
- **Data Processing**: Apache Spark, Dask, Ray, Apache Beam
- **Workflow Orchestration**: Apache Airflow, Prefect, Kubeflow

### Big Data Technologies
- **Streaming**: Apache Kafka, Apache Pulsar, Amazon Kinesis
- **Batch Processing**: Apache Spark, Hadoop MapReduce
- **Data Storage**: HDFS, Amazon S3, Azure Data Lake, Google Cloud Storage
- **Data Formats**: Parquet, Avro, ORC, Delta Lake

### Cloud & Infrastructure
- **AWS**: S3, EMR, Glue, Kinesis, SageMaker, Redshift
- **Azure**: Data Factory, Synapse, Stream Analytics, ML Studio
- **GCP**: Dataflow, BigQuery, Cloud ML Engine, Dataproc
- **Containerization**: Docker, Kubernetes for data workloads

### Databases & Storage
- **SQL Databases**: PostgreSQL, MySQL, SQL Server
- **NoSQL**: MongoDB, Cassandra, HBase, DynamoDB
- **Time-Series**: InfluxDB, TimescaleDB, OpenTSDB
- **Vector Databases**: Pinecone, Weaviate, Chroma, Milvus

## Daily Activities

### Morning (9:00 AM - 12:00 PM)
- Monitor data pipeline health and performance metrics
- Review data quality reports and anomaly detection alerts
- Collaborate with data scientists on feature requirements
- Work on data pipeline optimization and troubleshooting

### Afternoon (1:00 PM - 5:00 PM)
- Develop new data pipelines for ML projects
- Implement feature engineering and data transformation logic
- Test and validate data processing workflows
- Coordinate with ML engineers on data integration

### Evening (5:00 PM - 6:00 PM)
- Update data documentation and lineage tracking
- Plan data infrastructure improvements
- Review data governance and compliance metrics

## Goals & Success Metrics

### Short-term Goals (1-3 months)
- Achieve 99.5% data pipeline reliability
- Reduce feature engineering time by 40%
- Implement comprehensive data quality monitoring
- Complete migration to cloud-native data infrastructure

### Long-term Goals (6-12 months)
- Lead implementation of enterprise feature store
- Establish real-time ML inference data pipelines
- Achieve full data lineage tracking across all ML projects
- Implement automated data testing and validation

### Key Performance Indicators
- Data pipeline uptime and reliability
- Feature engineering pipeline performance
- Data quality scores and anomaly detection
- Time from data ingestion to ML model availability

## Technical Expertise

### ML Data Pipeline Architecture
- Stream processing for real-time ML features
- Batch processing for model training datasets
- Feature store design and implementation
- Data versioning and experiment reproducibility

### Data Quality & Monitoring
- Automated data validation and testing
- Data drift and distribution monitoring
- Anomaly detection in data pipelines
- Data lineage and impact analysis

### Performance Optimization
- Query optimization for large-scale data processing
- Caching strategies for frequently accessed features
- Parallel processing and distributed computing
- Resource allocation and cost optimization

## Work Environment Preferences
- **Schedule**: Standard business hours with on-call rotation
- **Location**: Hybrid work (60% remote, 40% office)
- **Collaboration**: Daily sync with data science and ML engineering teams
- **Focus Time**: Prefers morning hours for complex pipeline development
- **Tools**: High-performance workstation, multiple monitors, data visualization tools
